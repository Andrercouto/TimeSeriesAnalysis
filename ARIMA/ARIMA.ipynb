{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fa47e3",
   "metadata": {},
   "source": [
    "# ARIMA (SARIMA)\n",
    "\n",
    "The script aims to forecast the values of a time series using an ARIMA (or SARIMA) model. The series in question, which can be found on this [link](https://www3.bcb.gov.br/sgspub/consultarvalores/telaCvsSelecionarSeries.paint) represents the Retail Sales Volume Index in the state of São Paulo, Brazil, with monthly data from 2000 to 2022.\n",
    "\n",
    "The model will be trained using the data up to December 2020 and then be capable of making predictions for the years 2021 and 2022.\n",
    "\n",
    "The chosen evaluation metric is MAPE (Mean Absolute Percentage Error), which is commonly used to evaluate forecasts in time series. MAPE is calculated as the average of the absolute percentage differences between actual and predicted values relative to actual values.\n",
    "\n",
    "The formula for MAPE is given by:\n",
    "\n",
    "$MAPE = \\large\\frac{1}{n}\\small * ∑\\large|\\large\\frac{(Yi - Pi)}{Yi}|\\small * 100$\n",
    "\n",
    "Where:\n",
    "\n",
    "- n is the number of observations\n",
    "- $Y_i$ is the actual value of observation i\n",
    "- $P_i$ is the predicted value of observation i\n",
    "\n",
    "## Implementation\n",
    "\n",
    "- Check the dataset for patterns of trend and seasonality.\n",
    "\n",
    "\n",
    "- Test the stationarity of the data using the Dickey-Fuller test.\n",
    "\n",
    "\n",
    "- Perform differencing on the series.\n",
    "\n",
    "\n",
    "- Create and train the model.\n",
    "\n",
    "\n",
    "- Obtain the forecasts.\n",
    "\n",
    "\n",
    "- Perform reverse differencing.\n",
    "\n",
    "\n",
    "- Analyze the obtained results (outputs).\n",
    "\n",
    "***\n",
    "\n",
    "### Checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b945ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from pmdarima.arima.utils import ndiffs\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f4a7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/retailsp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24004\\3466688649.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Checking the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_retail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Data/retailsp.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_retail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/retailsp.csv'"
     ]
    }
   ],
   "source": [
    "# Checking the data\n",
    "df_retail = pd.read_csv(f'Data/retailsp.csv', index_col=0)\n",
    "df_retail.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Date column\n",
    "df_retail['Date'] = pd.to_datetime(df_retail['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Retail Value over the Date\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_retail['Date'], df_retail['value'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Retail value')\n",
    "plt.title('Retail value over the Date')\n",
    "plt.axvline(dt.datetime(2021, 1, 1), color='red', linestyle='dotted')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b032030",
   "metadata": {},
   "source": [
    "The dotted line indicates the point at which the forecasts will begin to be made.\n",
    "\n",
    "By looking at the plot, it is clearly evident that there is a seasonal pattern in the data (which would indicate that the SARIMA model should be used instead of ARIMA). However, it might be interesting to perform a double-check by decomposing the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac742bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposing\n",
    "decomposed_ts = sm.tsa.seasonal_decompose(df_retail['value'], period=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Seasonality\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(df_retail['Date'], decomposed_ts.seasonal)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Seasonality')\n",
    "plt.axvline(dt.datetime(2021, 1, 1), color='red', linestyle='dotted')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eeeb76",
   "metadata": {},
   "source": [
    "Decomposing the time series makes it even clearer that there is a seasonal component influencing the series on an annual basis, and this information should be considered in the model construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16bc9c",
   "metadata": {},
   "source": [
    "### Verifying stationarity\n",
    "\n",
    "A stationary time series is one in which the statistical properties, such as mean, variance, and autocorrelation, do not change over time. In other words, the series maintains a consistent pattern and is not affected by trends, seasonality, or abrupt changes. The ARIMA model can only be applied to stationary series.\n",
    "\n",
    "One way to test the stationarity of a series is by using the Dickey-Fuller test. This test helps determine whether a series is stationary by examining the presence of unit roots. If the test results in a p-value below a certain significance level (e.g., 0.05), it suggests that the series is stationary. Conversely, a p-value above the significance level indicates that the series is non-stationary and requires further treatment, such as differencing, to achieve stationarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dickey Fuller Test\n",
    "adf_result = adfuller(df_retail['value'], maxlag=1)\n",
    "adf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7e2f8",
   "metadata": {},
   "source": [
    "The second value in the print is the p-value, which, being greater than 0.05, does not reject the null hypothesis, indicating that the series, under a 5% significance level, is stationary.\n",
    "\n",
    "### Differencing the Series \n",
    "\n",
    "The series is not stationary; therefore, some form of treatment is necessary for applying the model.\n",
    "\n",
    "The most commonly used treatment to remove non-stationary trends and patterns is differencing, which makes the series more suitable for modeling using the AR (autoregressive) and MA (moving average) components of ARIMA.\n",
    "\n",
    "By applying differencing, it is possible to reduce the series' dependence on time and make it stationary in terms of mean and variance. This facilitates forecasting and modeling using ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the number of differentiations required\n",
    "diffs = ndiffs(df_retail['value'], test='adf')\n",
    "diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e1b6e",
   "metadata": {},
   "source": [
    "The series requires 1 differentiation, which means that it is necessary to subtract the values of the series over a time interval to make it stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing\n",
    "dif_df_retail = np.diff(df_retail['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb593b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the values after the differencing process\n",
    "print(dif_df_retail[0:5])\n",
    "print(df_retail['value'].loc[1:5].values -  df_retail['value'].loc[0:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13567c",
   "metadata": {},
   "source": [
    "As we can observe, the obtained values correspond to the subtraction of an original value by its previous value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977589ad",
   "metadata": {},
   "source": [
    "### Create and train the model\n",
    "\n",
    "It will be necessary to split the differenced series into training and testing sets. This is done to create and adjust the model and later make predictions that can be compared with the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the last value index\n",
    "idx_split = df_retail.loc[df_retail['Date']==dt.datetime(2020,12,1)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4718ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train and test values\n",
    "train_values = dif_df_retail[0:idx_split]\n",
    "test_values = dif_df_retail[idx_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c145c6c",
   "metadata": {},
   "source": [
    "The model will be created using the auto_arima function, which performs an automatic search to determine the best parameters for the ARIMA model based on the training data. The 'trace' parameter is set to True to display information about the model.\n",
    "\n",
    "The seasonal parameter, previously mentioned, should be added to the function, and 'm=12' sets the seasonal periodicity as 12, indicating an annual seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db10d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "arima_model = auto_arima(train_values, trace=True, seazonal=True, m=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5d4c0",
   "metadata": {},
   "source": [
    "The ARIMA(1,0,2)(0,1,1)[12] model was selected as the best model based on the automatic search performed. This model indicates that it has an autoregressive (AR) component of order 1, a moving average (MA) component of order 2, and a seasonal differencing component of order 1 with a period of 12 (indicating annual seasonality). These parameters were determined to be the most suitable for fitting the training data based on the Akaike Information Criterion (AIC).\n",
    "\n",
    "After the selection, the model will be fitted to the training data using the determined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eddf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "arima_model.fit(train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Outputs\n",
    "arima_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc614cca",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "The now fitted model will make predictions corresponding to the size of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1abfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = arima_model.predict(n_periods=len(test_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cf22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf974f6",
   "metadata": {},
   "source": [
    "Although the model's predictions may seem consistent with the test data, it is important to remember that these data are differenced and, therefore, the predictions do not correspond to the actual values of the dataset.\n",
    "\n",
    "The usefulness of the model will need to be assessed after the reverse differencing procedure, where the predicted values can be compared with the actual values.\n",
    "\n",
    "### Reverse differencing\n",
    "\n",
    "The way to obtain reverse differentiation is by taking the last value of the training series and adding the values obtained from the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be04926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train database last value\n",
    "train_last_value = df_retail['value'].loc[idx_split]\n",
    "train_last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528acd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the reversed predictions\n",
    "r_pred = np.r_[train_last_value, y_pred].cumsum()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b588057",
   "metadata": {},
   "source": [
    "### Analyzing the results\n",
    "\n",
    "Once the predicted values have undergone the reverse differentiation process, their values can be compared with those of the training dataset.\n",
    "\n",
    "The Mean Absolute Percentage Error (MAPE) will be the metric used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc138a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted and real values\n",
    "df_test = df_retail.loc[idx_split+1:]\n",
    "df_test['predicted_value'] = r_pred\n",
    "df_test[['Date', 'value', 'predicted_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6583cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting MAPE\n",
    "print(f\"MAPE: {round((np.mean(abs((df_test['value'] - df_test['predicted_value'])/df_test['value']))),4) * 100}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbf084",
   "metadata": {},
   "source": [
    "A MAPE of 5.91% indicates that, on average, the model's predictions have a mean absolute percentage error of 5.91% compared to the actual values in the training dataset. A lower MAPE value indicates better model performance, as it indicates that the predictions are closer to the actual values. Therefore, a MAPE of 5.91% is considered a positive result, indicating good accuracy in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518df448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Retail Value and predictions after 2020\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_retail['Date'].loc[df_retail['Date']>dt.datetime(2020, 1, 1)], df_retail['value'].loc[df_retail['Date']>dt.datetime(2020, 1, 1)])\n",
    "plt.plot(df_test['Date'], df_test['predicted_value'])\n",
    "plt.xlabel('Date')\n",
    "plt.title('Retail Value and Predictions')\n",
    "plt.axvline(dt.datetime(2021, 1, 1), color='red', linestyle='dotted')\n",
    "plt.legend(['Real values', 'Predicted values'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34550be9",
   "metadata": {},
   "source": [
    "As we can see from the graph, the model made accurate predictions of the Retail Index for the state of São Paulo.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "The conclusion that can be drawn from applying the ARIMA model to the dataset is that the final predictions were good, with a mean error of 5.91%. Some information, such as the presence of seasonality and the verification that the data needed to be differenced using the Dickey Fuller test, was necessary for the optimal fit of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
